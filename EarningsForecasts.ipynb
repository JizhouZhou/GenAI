{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/JizhouZhou/GenAI/blob/main/EarningsForecasts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoVEJ_XJ6sDU",
    "outputId": "fc996393-1c5e-44fb-8ce3-890b0e09cb12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /Users/zhou/PyVENV/lib/python3.9/site-packages (0.14.4)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /Users/zhou/PyVENV/lib/python3.9/site-packages (from statsmodels) (1.26.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /Users/zhou/PyVENV/lib/python3.9/site-packages (from statsmodels) (1.12.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /Users/zhou/PyVENV/lib/python3.9/site-packages (from statsmodels) (2.2.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /Users/zhou/PyVENV/lib/python3.9/site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/zhou/PyVENV/lib/python3.9/site-packages (from statsmodels) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/zhou/PyVENV/lib/python3.9/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zhou/PyVENV/lib/python3.9/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/zhou/PyVENV/lib/python3.9/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/zhou/PyVENV/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  !pip install statsmodels\n",
    "except:\n",
    "  pass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.offsets import *\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hEQZIw5Q6sDV"
   },
   "outputs": [],
   "source": [
    "ratio_chars = ['CAPEI', 'bm',\n",
    "       'evm', 'pe_exi', 'pe_inc', 'ps', 'pcf',\n",
    "       'dpr', 'npm', 'opmbd', 'opmad', 'gpm', 'ptpm', 'cfm', 'roa', 'roe',\n",
    "       'roce', 'efftax', 'aftret_eq', 'aftret_invcapx', 'aftret_equity',\n",
    "       'pretret_noa', 'pretret_earnat', 'GProf', 'equity_invcap',\n",
    "       'debt_invcap', 'totdebt_invcap', 'capital_ratio', 'int_debt',\n",
    "       'int_totdebt', 'cash_lt', 'invt_act', 'rect_act', 'debt_at',\n",
    "       'debt_ebitda', 'short_debt', 'curr_debt', 'lt_debt', 'profit_lct',\n",
    "       'ocf_lct', 'cash_debt', 'fcf_ocf', 'lt_ppent', 'dltt_be', 'debt_assets',\n",
    "       'debt_capital', 'de_ratio', 'intcov', 'intcov_ratio', 'cash_ratio',\n",
    "       'quick_ratio', 'curr_ratio', 'cash_conversion', 'inv_turn', 'at_turn',\n",
    "       'rect_turn', 'pay_turn', 'sale_invcap', 'sale_equity', 'sale_nwc',\n",
    "       'rd_sale', 'adv_sale', 'staff_sale', 'accrual', 'ptb', 'PEG_trailing',\n",
    "       'divyield']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9q6GCR86sDV"
   },
   "source": [
    "# RF with look-ahead bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YnNMNLoE61z_",
    "outputId": "ff57ec40-95b6-4860-d31b-ce08ae293b3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z0O6lvSG6sDW"
   },
   "outputs": [],
   "source": [
    "df_tmp = pd.read_parquet('./data/Results/df_train_new.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OX70_Itk6sDW",
    "outputId": "88d2a503-1997-4c4c-9ac3-53a8dadeb90e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling Forecast: 100%|█████████████████████| 408/408 [3:32:15<00:00, 31.21s/it]\n"
     ]
    }
   ],
   "source": [
    "forecast = []\n",
    "time_idx = sorted(df_tmp['YearMonth'].unique())\n",
    "time_idx = [i for i in time_idx if i > pd.to_datetime('1986-01-01')]\n",
    "num_trees = 2000\n",
    "\n",
    "for t in tqdm(time_idx, desc=\"Rolling Forecast\"):\n",
    "    forecast_dict = {}  # Use a dictionary instead of locals()\n",
    "\n",
    "    for q in ['q1', 'q2', 'q3', 'y1', 'y2']:\n",
    "        if q in ['q1', 'q2', 'q3']:\n",
    "            months_back = 12\n",
    "        else:\n",
    "            months_back = 24 if q == 'y2' else 12\n",
    "\n",
    "        x_cols = ratio_chars + ['ret', 'prc', f'EPS_true_l1_{q}', f'EPS_ana_{q}'] + ['RGDP', 'RCON', 'INDPROD', 'UNEMP']\n",
    "        y_col = f'EPS_true_{q}'\n",
    "        ann_date_col = f'ANNDATS_{q}'\n",
    "\n",
    "        # Training data\n",
    "        df_train = df_tmp[\n",
    "            (df_tmp['YearMonth'] < t) &\n",
    "            (df_tmp['YearMonth'] >= t - MonthEnd(months_back)) &\n",
    "            (df_tmp[ann_date_col] + MonthEnd(0) < t)\n",
    "        ].copy()\n",
    "\n",
    "        # Test data\n",
    "        df_test = df_tmp[\n",
    "            (df_tmp[ann_date_col] > df_tmp['YearMonth']) &\n",
    "            (df_tmp['YearMonth'] == t)\n",
    "        ].copy()\n",
    "\n",
    "        # Drop NA and infinite values\n",
    "        df_train = df_train.replace([np.inf, -np.inf], np.nan).dropna(subset=x_cols + [y_col])\n",
    "        df_test = df_test.replace([np.inf, -np.inf], np.nan).dropna(subset=x_cols + [y_col])\n",
    "\n",
    "        if df_train.empty or df_test.empty:\n",
    "            continue\n",
    "\n",
    "        # Fit model\n",
    "        mdl = RandomForestRegressor(\n",
    "            n_estimators=num_trees,\n",
    "            random_state=0,\n",
    "            max_depth=7,\n",
    "            min_samples_leaf=5,\n",
    "            max_samples=0.05,\n",
    "            n_jobs=14\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            mdl.fit(df_train[x_cols], df_train[y_col])\n",
    "            y_pred = mdl.predict(df_test[x_cols])\n",
    "        except ValueError as e:\n",
    "            print(f\"Error at time {t} in quarter {q}: {e}\")\n",
    "            continue\n",
    "\n",
    "        forecast_df = pd.DataFrame({\n",
    "            'permno': df_test['permno'],\n",
    "            'YearMonth': df_test['YearMonth'],\n",
    "            f'RF_{q}': y_pred,\n",
    "            f'AF_{q}': df_test[f'EPS_ana_{q}'],\n",
    "            f'AE_{q}': df_test[f'EPS_true_{q}']\n",
    "        })\n",
    "\n",
    "        forecast_dict[q] = forecast_df\n",
    "\n",
    "    # Combine forecasts for this time t\n",
    "    forecasts_to_merge = [forecast_dict[q] for q in ['q1', 'q2', 'q3', 'y1', 'y2'] if q in forecast_dict]\n",
    "    if forecasts_to_merge:\n",
    "        try:\n",
    "            forecast_combined = reduce(lambda x, y: pd.merge(x, y, on=['permno', 'YearMonth'], how='outer'),\n",
    "                                       forecasts_to_merge)\n",
    "            forecast.append(forecast_combined)\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging forecasts for time {t}: {e}\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"No forecast data available for merging at time {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8n2JbFYi6sDW"
   },
   "outputs": [],
   "source": [
    "forecast_all = pd.concat(forecast,axis=0).reset_index()\n",
    "forecast_all.to_parquet('./data/Results/RF_with_lookahead_raw_005.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srIoPDZ_6sDW"
   },
   "source": [
    "# RF without look-ahead bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "trTu1ihi6sDW"
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "## Difference in predictors ##\n",
    "##############################\n",
    "# Q1: same\n",
    "# Q2: 'EPS_true_l1_q2' --> 'EPS_true_l1_q1'\n",
    "# Q3: 'EPS_true_l1_q3' --> 'EPS_true_l1_q1'\n",
    "# Y1: same\n",
    "# Y2: 'EPS_true_l1_y2' --> 'EPS_true_l1_y1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AbUAbUSW6sDW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/408 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 75)) while a minimum of 1 is required by RandomForestRegressor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 86\u001b[0m\n\u001b[1;32m     77\u001b[0m df_test \u001b[38;5;241m=\u001b[39m df_tmp[(df_tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANNDATS_y1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m>\u001b[39mdf_tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYearMonth\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m&\u001b[39m (df_tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYearMonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m t)]\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39mx_cols\u001b[38;5;241m+\u001b[39m[y_col])\n\u001b[1;32m     79\u001b[0m mdl \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39mnum_trees,\n\u001b[1;32m     80\u001b[0m                          random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     81\u001b[0m                          max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m,\n\u001b[1;32m     82\u001b[0m                          min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     83\u001b[0m                          max_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[1;32m     84\u001b[0m                          n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m \u001b[43mmdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43my_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m mdl\u001b[38;5;241m.\u001b[39mpredict(df_test[x_cols])\n\u001b[1;32m     89\u001b[0m forecast_y1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpermno\u001b[39m\u001b[38;5;124m'\u001b[39m:df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpermno\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYearMonth\u001b[39m\u001b[38;5;124m'\u001b[39m:df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYearMonth\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     90\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRF_y1\u001b[39m\u001b[38;5;124m'\u001b[39m:y_pred, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAF_y1\u001b[39m\u001b[38;5;124m'\u001b[39m:df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPS_ana_y1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAE_y1\u001b[39m\u001b[38;5;124m'\u001b[39m:df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPS_true_y1\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[0;32m~/PyVENV/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PyVENV/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:360\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 360\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[0;32m~/PyVENV/lib/python3.9/site-packages/sklearn/utils/validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/PyVENV/lib/python3.9/site-packages/sklearn/utils/validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[0;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/PyVENV/lib/python3.9/site-packages/sklearn/utils/validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1133\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1134\u001b[0m         )\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1137\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 75)) while a minimum of 1 is required by RandomForestRegressor."
     ]
    }
   ],
   "source": [
    "## Rolling Window:\n",
    "time_idx = sorted(df_tmp['YearMonth'].unique())\n",
    "time_idx = [i for i in time_idx if i > pd.to_datetime('1986-01-01')]\n",
    "\n",
    "forecast = []\n",
    "for t in tqdm(time_idx):\n",
    "\n",
    "    ### Q1 ###\n",
    "    x_cols = ratio_chars + ['ret','prc','EPS_true_l1_q1','EPS_ana_q1'] + ['RGDP', 'RCON', 'INDPROD', 'UNEMP']\n",
    "    y_col = 'EPS_true_q1'\n",
    "\n",
    "    df_train = df_tmp[(df_tmp['YearMonth'] < t) & (df_tmp['YearMonth'] >= t - MonthEnd(12)) & (df_tmp['ANNDATS_q1'] + MonthEnd(0) < t)]\\\n",
    "               .dropna(subset=x_cols+[y_col])\n",
    "    df_test = df_tmp[(df_tmp['ANNDATS_q1']>df_tmp['YearMonth']) & (df_tmp['YearMonth'] == t)].dropna(subset=x_cols+[y_col])\n",
    "    # break\n",
    "    mdl = RandomForestRegressor(n_estimators=num_trees,\n",
    "                             random_state=0,\n",
    "                             max_depth=7,\n",
    "                             min_samples_leaf=5,\n",
    "                             max_samples=0.05,\n",
    "                             n_jobs=14)\n",
    "\n",
    "    mdl.fit(df_train[x_cols], df_train[y_col])\n",
    "    y_pred = mdl.predict(df_test[x_cols])\n",
    "\n",
    "    forecast_q1 = pd.DataFrame({'permno':df_test['permno'],'YearMonth':df_test['YearMonth'],\n",
    "                                'RF_q1':y_pred, 'AF_q1':df_test['EPS_ana_q1'], 'AE_q1':df_test['EPS_true_q1']})\n",
    "\n",
    "    ### Q2 ###\n",
    "    x_cols = ratio_chars + ['ret','prc','EPS_true_l1_q1','EPS_ana_q2'] + ['RGDP', 'RCON', 'INDPROD', 'UNEMP']\n",
    "    y_col = 'EPS_true_q2'\n",
    "\n",
    "    df_train = df_tmp[(df_tmp['YearMonth'] < t) & (df_tmp['YearMonth'] >= t - MonthEnd(12)) & (df_tmp['ANNDATS_q2']  + MonthEnd(0) < t)]\\\n",
    "               .dropna(subset=x_cols+[y_col])\n",
    "    df_test = df_tmp[(df_tmp['ANNDATS_q2']>df_tmp['YearMonth']) & (df_tmp['YearMonth'] == t)].dropna(subset=x_cols+[y_col])\n",
    "\n",
    "    mdl = RandomForestRegressor(n_estimators=num_trees,\n",
    "                             random_state=0,\n",
    "                             max_depth=7,\n",
    "                             min_samples_leaf=5,\n",
    "                             max_samples=0.05,\n",
    "                             n_jobs=14)\n",
    "\n",
    "    mdl.fit(df_train[x_cols], df_train[y_col])\n",
    "    y_pred = mdl.predict(df_test[x_cols])\n",
    "\n",
    "    forecast_q2 = pd.DataFrame({'permno':df_test['permno'],'YearMonth':df_test['YearMonth'],\n",
    "                                'RF_q2':y_pred, 'AF_q2':df_test['EPS_ana_q2'], 'AE_q2':df_test['EPS_true_q2']})\n",
    "\n",
    "    ### Q3 ###\n",
    "    x_cols = ratio_chars + ['ret','prc','EPS_true_l1_q1','EPS_ana_q3'] + ['RGDP', 'RCON', 'INDPROD', 'UNEMP']\n",
    "    y_col = 'EPS_true_q3'\n",
    "\n",
    "    df_train = df_tmp[(df_tmp['YearMonth'] < t) & (df_tmp['YearMonth'] >= t - MonthEnd(12)) & (df_tmp['ANNDATS_q3']  + MonthEnd(0) < t)]\\\n",
    "               .dropna(subset=x_cols+[y_col])\n",
    "    df_test = df_tmp[(df_tmp['ANNDATS_q3']>df_tmp['YearMonth']) & (df_tmp['YearMonth'] == t)].dropna(subset=x_cols+[y_col])\n",
    "\n",
    "    mdl = RandomForestRegressor(n_estimators=num_trees,\n",
    "                             random_state=0,\n",
    "                             max_depth=7,\n",
    "                             min_samples_leaf=5,\n",
    "                             max_samples=0.05,\n",
    "                             n_jobs=14)\n",
    "\n",
    "    mdl.fit(df_train[x_cols], df_train[y_col])\n",
    "    y_pred = mdl.predict(df_test[x_cols])\n",
    "\n",
    "    forecast_q3 = pd.DataFrame({'permno':df_test['permno'],'YearMonth':df_test['YearMonth'],\n",
    "                                'RF_q3':y_pred, 'AF_q3':df_test['EPS_ana_q3'], 'AE_q3':df_test['EPS_true_q3']})\n",
    "\n",
    "    ### Y1 ###\n",
    "    x_cols = ratio_chars + ['ret','prc','EPS_true_l1_y1','EPS_ana_y1'] + ['RGDP', 'RCON', 'INDPROD', 'UNEMP']\n",
    "    y_col = 'EPS_true_y1'\n",
    "\n",
    "    df_train = df_tmp[(df_tmp['YearMonth'] < t) & (df_tmp['YearMonth'] >= t - MonthEnd(12)) & (df_tmp['ANNDATS_y1']  + MonthEnd(0) < t)]\\\n",
    "               .dropna(subset=x_cols+[y_col])\n",
    "    df_test = df_tmp[(df_tmp['ANNDATS_y1']>df_tmp['YearMonth']) & (df_tmp['YearMonth'] == t)].dropna(subset=x_cols+[y_col])\n",
    "\n",
    "    mdl = RandomForestRegressor(n_estimators=num_trees,\n",
    "                             random_state=0,\n",
    "                             max_depth=7,\n",
    "                             min_samples_leaf=5,\n",
    "                             max_samples=0.05,\n",
    "                             n_jobs=14)\n",
    "\n",
    "    mdl.fit(df_train[x_cols], df_train[y_col])\n",
    "    y_pred = mdl.predict(df_test[x_cols])\n",
    "\n",
    "    forecast_y1 = pd.DataFrame({'permno':df_test['permno'],'YearMonth':df_test['YearMonth'],\n",
    "                                'RF_y1':y_pred, 'AF_y1':df_test['EPS_ana_y1'], 'AE_y1':df_test['EPS_true_y1']})\n",
    "\n",
    "    ### Y2 ###\n",
    "    x_cols = ratio_chars + ['ret','prc','EPS_true_l1_y1','EPS_ana_y2'] + ['RGDP', 'RCON', 'INDPROD', 'UNEMP']\n",
    "    y_col = 'EPS_true_y2'\n",
    "\n",
    "    df_train = df_tmp[(df_tmp['YearMonth'] < t) & (df_tmp['YearMonth'] >= t - MonthEnd(24)) & (df_tmp['ANNDATS_y2']  + MonthEnd(0) < t)]\\\n",
    "               .dropna(subset=x_cols+[y_col])\n",
    "    df_test = df_tmp[(df_tmp['ANNDATS_y2']>df_tmp['YearMonth']) & (df_tmp['YearMonth'] == t)].dropna(subset=x_cols+[y_col])\n",
    "\n",
    "    mdl = RandomForestRegressor(n_estimators=num_trees,\n",
    "                             random_state=0,\n",
    "                             max_depth=7,\n",
    "                             min_samples_leaf=5,\n",
    "                             max_samples=0.05,\n",
    "                             n_jobs=14)\n",
    "\n",
    "    mdl.fit(df_train[x_cols], df_train[y_col])\n",
    "    y_pred = mdl.predict(df_test[x_cols])\n",
    "\n",
    "    forecast_y2 = pd.DataFrame({'permno':df_test['permno'],'YearMonth':df_test['YearMonth'],\n",
    "                                'RF_y2':y_pred, 'AF_y2':df_test['EPS_ana_y2'], 'AE_y2':df_test['EPS_true_y2']})\n",
    "\n",
    "    forecast.append(reduce(lambda x,y: pd.merge(x,y,\n",
    "                                       on=['permno','YearMonth'],\n",
    "                                       how='outer'),\n",
    "                 [forecast_q1,forecast_q2,forecast_q3,\n",
    "                  forecast_y1,forecast_y2]))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbivekjj6sDX"
   },
   "outputs": [],
   "source": [
    "forecast_all = pd.concat(forecast,axis=0).reset_index()\n",
    "forecast_all.to_parquet('./data/Results/RF_wo_lookahead_raw_005.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5IEHUz26sDX"
   },
   "source": [
    "# Hughes et al.(2008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrafH3O06sDX"
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvYLtPBc6sDX"
   },
   "outputs": [],
   "source": [
    "###  Characteristics ###\n",
    "\n",
    "# acc, dPPE, dOLA, sg_5y from Compustat\n",
    "compa = pd.read_parquet('../data/WRDS/compa.parquet')\n",
    "compa['gvkey'] = compa['gvkey'].astype(float)\n",
    "compa['datadate'] = compa['datadate'] + MonthEnd(0)\n",
    "compa = compa[compa['at_avg'] > 0].copy()\n",
    "compa['acc'] = compa['acc']/compa['at_avg']\n",
    "compa['dPPE'] = compa['ppegt_diff']/compa['at_avg']\n",
    "compa['dOLA'] = compa['ao_diff']/compa['at_avg']\n",
    "compa['sg_5y'] = np.nan_to_num(compa['sg_5y'], nan=np.nan, posinf=np.nan, neginf=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCOrcr4-6sDX"
   },
   "outputs": [],
   "source": [
    "# LTG from IBES\n",
    "consensus = pd.read_parquet('../data/WRDS/EPS_summary.parquet')\n",
    "consensus['YearMonth'] = consensus.statpers + MonthEnd(0)\n",
    "consensus_ltg = consensus[consensus.fpi == '0'].copy()\n",
    "consensus_ltg['LTG'] = consensus_ltg['meanest']\n",
    "consensus_ltg = consensus_ltg[['ticker','YearMonth','LTG']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPvQikZn6sDX"
   },
   "outputs": [],
   "source": [
    "# Forecast Revision from IBES\n",
    "\n",
    "# Analyst Forecast Earnings:\n",
    "crsp = pd.read_parquet('../data/WRDS/crsp_m.parquet')\n",
    "crsp['ME'] = abs(crsp['prc']) * crsp['shrout']\n",
    "consensus = consensus.merge(crsp[['permno','ncusip','YearMonth','shrout']],\n",
    "                            left_on=['cusip','YearMonth'],\n",
    "                            right_on=['ncusip','YearMonth'])\n",
    "consensus['AF'] = consensus['meanest'] * consensus['shrout']\n",
    "\n",
    "## Revision for each Annual horizon\n",
    "result = []\n",
    "for i in [1,2]:\n",
    "    consensus_f1 = consensus[consensus.fpi=='{}'.format(i)].copy()\n",
    "    consensus_f1.drop_duplicates(subset=['YearMonth','ticker'], inplace=True)\n",
    "\n",
    "    consensus_f1_lag = consensus[consensus.fpi.isin(['{}'.format(i),\n",
    "                                                     '{}'.format(i+1)])][['YearMonth','ticker',\n",
    "                                                                          'fpedats','AF']].copy()\n",
    "\n",
    "    # Forecast in last month: F_{t-1}[x_t]\n",
    "    consensus_f1_lag['YearMonth'] = consensus_f1_lag['YearMonth'] + MonthEnd(1)\n",
    "    consensus_f1_lag.rename(columns={'AF':'AF_l1'},inplace=True)\n",
    "\n",
    "    consensus_f1_change = consensus_f1[['YearMonth','permno','ticker','fpedats','AF']].merge(consensus_f1_lag,\n",
    "                                                                                        on=['YearMonth','ticker','fpedats'],\n",
    "                                                                                        how='left')\n",
    "\n",
    "    consensus_f1_change['FRevision_A{}'.format(i)] = consensus_f1_change['AF'] - \\\n",
    "                                                     consensus_f1_change['AF_l1']\n",
    "    consensus_f1_change.drop_duplicates(subset=['YearMonth','permno'], inplace=True)\n",
    "    result.append(consensus_f1_change[['permno','YearMonth','FRevision_A{}'.format(i)]])\n",
    "\n",
    "FR = reduce(lambda x,y: pd.merge(x,y,on=['permno','YearMonth'], how='outer'), result)\n",
    "FR.sort_values(by=['permno','YearMonth'],inplace=True,ignore_index=True)\n",
    "# Revision in recent 3months\n",
    "FR[['FRevision_A1_3m','FRevision_A2_3m']] = FR.groupby('permno')[['FRevision_A1','FRevision_A2']]\\\n",
    "                                        .rolling(3).sum().reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBeejr-a6sDX"
   },
   "outputs": [],
   "source": [
    "# Earnings Surprise\n",
    "\n",
    "## Most recent Quarterly Earnings Announcement SUE\n",
    "# 1. Actual\n",
    "# IBES actual\n",
    "EPS_true = pd.read_stata('../data/WRDS/EPS_unadjusted_actual_full.dta')\n",
    "EPS_true['YearMonth'] = EPS_true['ANNDATS'] + MonthEnd(0)\n",
    "EPS_true = EPS_true.merge(crsp[['ncusip','YearMonth','shrout','cfacshr','ME']],\n",
    "                          left_on=['CUSIP','YearMonth'],\n",
    "                          right_on=['ncusip','YearMonth'])\n",
    "EPS_true['AE'] = EPS_true['VALUE'] * EPS_true['shrout']\n",
    "EPS_true['YearMonth'] = EPS_true['ANNDATS'] + MonthEnd(-1) # use this to merge with forecast\n",
    "EPS_true_qtr = EPS_true[EPS_true['PDICITY'] == 'QTR'].sort_values(by=['TICKER','PENDS'])\n",
    "# 2. Forecast in last month\n",
    "consensus_1q = consensus[consensus['fpi'] == '6']\n",
    "FE_last = EPS_true_qtr[['TICKER','ANNDATS','YearMonth','PENDS','AE','ME']].merge(consensus_1q[['ticker','YearMonth','fpedats','AF','statpers']],\n",
    "                   left_on=['TICKER','YearMonth','PENDS'],\n",
    "                   right_on=['ticker','YearMonth','fpedats'],\n",
    "                  )\n",
    "FE_last['SUE'] = (FE_last['AE'] - FE_last['AF']) / FE_last['ME']\n",
    "# These SUE shoulbe be used after the announcement\n",
    "FE_last['YearMonth'] = FE_last['ANNDATS'] + MonthEnd(0)\n",
    "FE_last.drop_duplicates(subset=['TICKER','YearMonth'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRm9URCi6sDX"
   },
   "outputs": [],
   "source": [
    "# Momentum\n",
    "crsp = pd.read_parquet('../data/WRDS/crsp_m.parquet')\n",
    "crsp.sort_values(by=['permno','YearMonth'], inplace=True)\n",
    "crsp['ret_12m'] = np.log(1 + crsp['ret']).groupby(crsp['permno'])\\\n",
    "                    .rolling(12).sum()\\\n",
    "                    .reset_index(level=0, drop=True)\n",
    "crsp['ret_12m'] = np.exp(crsp['ret_12m']) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rB2ZACXR6sDX"
   },
   "outputs": [],
   "source": [
    "df_tmp = pd.read_parquet('../data/Results/df_train_new.parquet')\n",
    "df_tmp = df_tmp.merge(compa, on=['gvkey','datadate'], how='left')\n",
    "df_tmp = df_tmp.merge(FR, on=['permno','YearMonth'], how='left')\n",
    "df_tmp = df_tmp.merge(consensus_ltg,\n",
    "                      left_on=['TICKER','YearMonth'],\n",
    "                      right_on=['ticker','YearMonth'],\n",
    "                      how='left')\n",
    "df_tmp = df_tmp.merge(FE_last[['TICKER','YearMonth','SUE']],\n",
    "                      on=['TICKER','YearMonth'],\n",
    "                      how='left'\n",
    "                     )\n",
    "df_tmp = df_tmp.merge(crsp[['permno','YearMonth','ret_12m']],\n",
    "                      on=['permno','YearMonth'],\n",
    "                      how='left'\n",
    "                     )\n",
    "df_tmp.sort_values(by=['permno','YearMonth'], inplace=True, ignore_index=True)\n",
    "df_tmp['SUE'] = df_tmp.groupby('permno')['SUE'].ffill(limit=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GA0QWOf6sDX"
   },
   "source": [
    "## Preprocess and Make Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NShsq7x_6sDY"
   },
   "outputs": [],
   "source": [
    "df_tmp['FRevision_A1_3m_std'] = df_tmp['FRevision_A1_3m']/df_tmp['ME']\n",
    "\n",
    "# winsorization period-by-period\n",
    "cols = [\n",
    "        'acc','LTG', 'sg_5y','dPPE', 'dOLA','ret_12m',\n",
    "        'SUE','FRevision_A1_3m_std'\n",
    "       ]\n",
    "\n",
    "df_tmp[cols] = df_tmp.groupby('YearMonth',group_keys=False)[cols]\\\n",
    "                     .transform(lambda x: x.clip(x.quantile(0.01),x.quantile(0.99)))\n",
    "\n",
    "# ## FillNA with Industry Median\n",
    "fillNA = ['acc','LTG', 'sg_5y','dPPE', 'dOLA','ret_12m',\n",
    "        'SUE','FRevision_A1_3m_std'\n",
    "       ]\n",
    "for v in tqdm(fillNA):\n",
    "    df_tmp[v] = df_tmp.groupby(['YearMonth','fama49'], group_keys=False)[v].apply(lambda x: x.fillna(x.median()))\n",
    "## In case some characteristics are all NA in some industry\n",
    "for v in tqdm(fillNA):\n",
    "    df_tmp[v] = df_tmp.groupby(['YearMonth'], group_keys=False)[v].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DR_9fAx-6sDY"
   },
   "outputs": [],
   "source": [
    "## Rolling Window:\n",
    "time_idx = sorted(df_tmp['YearMonth'].unique())\n",
    "time_idx = [i for i in time_idx if i > pd.to_datetime('1986-01-01')]\n",
    "forecast = []\n",
    "\n",
    "for t in tqdm(time_idx):\n",
    "\n",
    "    ### Q1 ###\n",
    "    x_cols = ['EPS_ana_q1','acc','LTG', 'sg_5y','dPPE', 'dOLA','SUE','ret_12m','FRevision_A1_3m_std']\n",
    "    y_col = 'EPS_true_q1'\n",
    "\n",
    "    df_train = df_tmp[(df_tmp['YearMonth'] < t) & (df_tmp['YearMonth'] >= t - MonthEnd(60)) & (df_tmp['ANNDATS_q1'] + MonthEnd(0) < t)]\\\n",
    "               .dropna(subset=x_cols+[y_col])\n",
    "    df_test = df_tmp[(df_tmp['ANNDATS_q1']>df_tmp['YearMonth']) & (df_tmp['YearMonth'] == t)].dropna(subset=x_cols+[y_col])\n",
    "    # break\n",
    "    mdl = sm.OLS(df_train[y_col], sm.add_constant(df_train[x_cols])).fit()\n",
    "    y_pred = mdl.predict(sm.add_constant(df_test[x_cols]))\n",
    "\n",
    "    forecast_q1 = pd.DataFrame({'permno':df_test['permno'],'YearMonth':df_test['YearMonth'],\n",
    "                                'LF_q1':y_pred, 'AF_q1':df_test['EPS_ana_q1'], 'AE_q1':df_test['EPS_true_q1']})\n",
    "\n",
    "    ### Q2 ###\n",
    "    x_cols = ['EPS_ana_q2','acc','LTG', 'sg_5y','dPPE', 'dOLA','SUE','ret_12m','FRevision_A1_3m_std']\n",
    "    y_col = 'EPS_true_q2'\n",
    "\n",
    "    df_train = df_tmp[(df_tmp['YearMonth'] < t) & (df_tmp['YearMonth'] >= t - MonthEnd(60)) & (df_tmp['ANNDATS_q2'] + MonthEnd(0) < t)]\\\n",
    "               .dropna(subset=x_cols+[y_col])\n",
    "    df_test = df_tmp[(df_tmp['ANNDATS_q2']>df_tmp['YearMonth']) & (df_tmp['YearMonth'] == t)].dropna(subset=x_cols+[y_col])\n",
    "    mdl = sm.OLS(df_train[y_col], sm.add_constant(df_train[x_cols])).fit()\n",
    "    y_pred = mdl.predict(sm.add_constant(df_test[x_cols]))\n",
    "\n",
    "    forecast_q2 = pd.DataFrame({'permno':df_test['permno'],'YearMonth':df_test['YearMonth'],\n",
    "                                'LF_q2':y_pred, 'AF_q2':df_test['EPS_ana_q2'], 'AE_q2':df_test['EPS_true_q2']})\n",
    "\n",
    "    ### Q3 ###\n",
    "    x_cols = ['EPS_ana_q3','acc','LTG', 'sg_5y','dPPE', 'dOLA','SUE','ret_12m','FRevision_A1_3m_std']\n",
    "    y_col = 'EPS_true_q3'\n",
    "\n",
    "    df_train = df_tmp[(df_tmp['YearMonth'] < t) & (df_tmp['YearMonth'] >= t - MonthEnd(60)) & (df_tmp['ANNDATS_q3'] + MonthEnd(0) < t)]\\\n",
    "               .dropna(subset=x_cols+[y_col])\n",
    "    df_test = df_tmp[(df_tmp['ANNDATS_q3']>df_tmp['YearMonth']) & (df_tmp['YearMonth'] == t)].dropna(subset=x_cols+[y_col])\n",
    "    mdl = sm.OLS(df_train[y_col], sm.add_constant(df_train[x_cols])).fit()\n",
    "    y_pred = mdl.predict(sm.add_constant(df_test[x_cols]))\n",
    "\n",
    "    forecast_q3 = pd.DataFrame({'permno':df_test['permno'],'YearMonth':df_test['YearMonth'],\n",
    "                                'LF_q3':y_pred, 'AF_q3':df_test['EPS_ana_q3'], 'AE_q3':df_test['EPS_true_q3']})\n",
    "\n",
    "    ### Y1 ###\n",
    "    x_cols = ['EPS_ana_y1','acc','LTG', 'sg_5y','dPPE', 'dOLA','SUE','ret_12m','FRevision_A1_3m_std']\n",
    "    y_col = 'EPS_true_y1'\n",
    "\n",
    "    df_train = df_tmp[(df_tmp['YearMonth'] < t) & (df_tmp['YearMonth'] >= t - MonthEnd(60)) & (df_tmp['ANNDATS_y1'] + MonthEnd(0) < t)]\\\n",
    "               .dropna(subset=x_cols+[y_col])\n",
    "    df_test = df_tmp[(df_tmp['ANNDATS_y1']>df_tmp['YearMonth']) & (df_tmp['YearMonth'] == t)].dropna(subset=x_cols+[y_col])\n",
    "    mdl = sm.OLS(df_train[y_col], sm.add_constant(df_train[x_cols])).fit()\n",
    "    y_pred = mdl.predict(sm.add_constant(df_test[x_cols]))\n",
    "\n",
    "    forecast_y1 = pd.DataFrame({'permno':df_test['permno'],'YearMonth':df_test['YearMonth'],\n",
    "                                'LF_y1':y_pred, 'AF_y1':df_test['EPS_ana_y1'], 'AE_y1':df_test['EPS_true_y1']})\n",
    "\n",
    "    ### Y2 ###\n",
    "    x_cols = ['EPS_ana_y2','acc','LTG', 'sg_5y','dPPE', 'dOLA','SUE','ret_12m','FRevision_A1_3m_std']\n",
    "    y_col = 'EPS_true_y2'\n",
    "\n",
    "    df_train = df_tmp[(df_tmp['YearMonth'] < t) & (df_tmp['YearMonth'] >= t - MonthEnd(60)) & (df_tmp['ANNDATS_y2'] + MonthEnd(0) < t)]\\\n",
    "               .dropna(subset=x_cols+[y_col])\n",
    "    df_test = df_tmp[(df_tmp['ANNDATS_y2']>df_tmp['YearMonth']) & (df_tmp['YearMonth'] == t)].dropna(subset=x_cols+[y_col])\n",
    "    mdl = sm.OLS(df_train[y_col], sm.add_constant(df_train[x_cols])).fit()\n",
    "    y_pred = mdl.predict(sm.add_constant(df_test[x_cols]))\n",
    "\n",
    "    forecast_y2 = pd.DataFrame({'permno':df_test['permno'],'YearMonth':df_test['YearMonth'],\n",
    "                                'LF_y2':y_pred, 'AF_y2':df_test['EPS_ana_y2'], 'AE_y2':df_test['EPS_true_y2']})\n",
    "\n",
    "    forecast.append(reduce(lambda x,y: pd.merge(x,y,\n",
    "                                       on=['permno','YearMonth'],\n",
    "                                       how='outer'),\n",
    "                 [forecast_q1,forecast_q2,forecast_q3,\n",
    "                  forecast_y1,forecast_y2]))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdzB1Jjh6sDY"
   },
   "outputs": [],
   "source": [
    "forecast_all = pd.concat(forecast,axis=0).reset_index()\n",
    "forecast_all.to_parquet('../data/Results/Hughes_eps.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9SXyb4e6sDY"
   },
   "source": [
    "# So (2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfpvZj1I6sDY"
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUDfsplp6sDY"
   },
   "outputs": [],
   "source": [
    "df_tmp = pd.read_parquet('../data/Results/df_train_new.parquet')\n",
    "# compustat data\n",
    "compa = pd.read_parquet('../data/WRDS/compa.parquet')\n",
    "compa['gvkey'] = compa['gvkey'].astype(float)\n",
    "compa['datadate'] = compa['datadate'] + MonthEnd(0)\n",
    "compa = compa[compa['at_avg'] > 0].copy()\n",
    "compa['acc'] = compa['acc']/compa['csho']\n",
    "compa['div'] = compa['dvc']/compa['csho']\n",
    "compa['dd'] = np.where(compa['dvc']==0, 1, 0)\n",
    "compa['acc_n'] = compa['acc'].clip(upper=0).abs()\n",
    "compa['acc_p'] = compa['acc'].clip(lower=0)\n",
    "\n",
    "df_tmp = df_tmp.merge(compa[['gvkey','datadate','dd','div',\n",
    "                             'acc_n','acc_p','ag',\n",
    "                            ]],\n",
    "                        left_on=['gvkey','adate'],\n",
    "                        right_on=['gvkey','datadate'],\n",
    "                        how='left')\n",
    "\n",
    "df_tmp['earnings_pos_l1_y1'] = df_tmp['EPS_true_l1_y1'].clip(lower=0)\n",
    "df_tmp['nege_l1_y1'] = np.where(df_tmp['EPS_true_l1_y1'] < 0, 1, 0)\n",
    "\n",
    "df_tmp['earnings_pos_l1_q1'] = df_tmp['EPS_true_l1_q1'].clip(lower=0)\n",
    "df_tmp['nege_l1_q1'] = np.where(df_tmp['EPS_true_l1_q1'] < 0, 1, 0)\n",
    "\n",
    "## winsorize period-by-period\n",
    "cols = ['earnings_pos_l1_y1','earnings_pos_l1_q1',\n",
    "        'acc_n','acc_p','ag','div',\n",
    "       ]\n",
    "df_tmp[cols] = df_tmp.groupby('YearMonth')[cols]\\\n",
    "                    .transform(lambda x: x.clip(x.quantile(0.01),x.quantile(0.99)))\n",
    "\n",
    "# ## FillNA with Industry Median\n",
    "fillNA = ['earnings_pos_l1_q1','nege_l1_q1',\n",
    "          'earnings_pos_l1_y1','nege_l1_y1',\n",
    "          'acc_n','acc_p','ag','dd','bm','prc','div']\n",
    "for v in tqdm(fillNA):\n",
    "    df_tmp[v] = df_tmp.groupby(['YearMonth','fama49'], group_keys=False)[v].apply(lambda x: x.fillna(x.median()))\n",
    "## In case some characteristics are all NA in some industry\n",
    "for v in tqdm(fillNA):\n",
    "    df_tmp[v] = df_tmp.groupby(['YearMonth'], group_keys=False)[v].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHsBkuuW6sDY"
   },
   "source": [
    "## Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVXmkHbc6sDY"
   },
   "outputs": [],
   "source": [
    "## Rolling Window:\n",
    "time_idx = sorted(df_tmp['YearMonth'].unique())\n",
    "time_idx = [i for i in time_idx if i > pd.to_datetime('1986-01-01')]\n",
    "\n",
    "params = {}\n",
    "t_values = {}\n",
    "\n",
    "forecast = []\n",
    "for t in tqdm(time_idx):\n",
    "\n",
    "    ### Q1 ###\n",
    "    x_cols = ['EPS_ana_q1','earnings_pos_l1_q1','nege_l1_q1','acc_n','acc_p','ag','dd','bm','prc','div']\n",
    "    y_col = 'EPS_true_q1'\n",
    "\n",
    "    df_train = df_tmp[(df_tmp['YearMonth'] < t) & (df_tmp['YearMonth'] >= t - MonthEnd(12)) & (df_tmp['ANNDATS_q1'] + MonthEnd(0) < t)]\\\n",
    "               .dropna(subset=x_cols+[y_col])\n",
    "    df_test = df_tmp[(df_tmp['ANNDATS_q1']>df_tmp['YearMonth']) & (df_tmp['YearMonth'] == t)].dropna(subset=x_cols+[y_col])\n",
    "    # break\n",
    "    mdl = LinearRegression()\n",
    "    mdl.fit(df_train[x_cols], df_train[y_col])\n",
    "    y_pred = mdl.predict(df_test[x_cols])\n",
    "\n",
    "    forecast_q1 = pd.DataFrame({'permno':df_test['permno'],'YearMonth':df_test['YearMonth'],\n",
    "                                'So_q1':y_pred, 'AF_q1':df_test['EPS_ana_q1'], 'AE_q1':df_test['EPS_true_q1']})\n",
    "\n",
    "    ### Q2 ###\n",
    "    x_cols = ['EPS_ana_q2','earnings_pos_l1_q1','nege_l1_q1','acc_n','acc_p','ag','dd','bm','prc','div']\n",
    "    y_col = 'EPS_true_q2'\n",
    "\n",
    "    df_train = df_tmp[(df_tmp['YearMonth'] < t) & (df_tmp['YearMonth'] >= t - MonthEnd(12)) & (df_tmp['ANNDATS_q2']  + MonthEnd(0) < t)]\\\n",
    "               .dropna(subset=x_cols+[y_col])\n",
    "    df_test = df_tmp[(df_tmp['ANNDATS_q2']>df_tmp['YearMonth']) & (df_tmp['YearMonth'] == t)].dropna(subset=x_cols+[y_col])\n",
    "\n",
    "    mdl = LinearRegression()\n",
    "\n",
    "    mdl.fit(df_train[x_cols], df_train[y_col])\n",
    "    y_pred = mdl.predict(df_test[x_cols])\n",
    "\n",
    "    forecast_q2 = pd.DataFrame({'permno':df_test['permno'],'YearMonth':df_test['YearMonth'],\n",
    "                                'So_q2':y_pred, 'AF_q2':df_test['EPS_ana_q2'], 'AE_q2':df_test['EPS_true_q2']})\n",
    "\n",
    "    ### Q3 ###\n",
    "    x_cols = ['EPS_ana_q3','earnings_pos_l1_q1','nege_l1_q1','acc_n','acc_p','ag','dd','bm','prc','div']\n",
    "    y_col = 'EPS_true_q3'\n",
    "\n",
    "    df_train = df_tmp[(df_tmp['YearMonth'] < t) & (df_tmp['YearMonth'] >= t - MonthEnd(12)) & (df_tmp['ANNDATS_q3']  + MonthEnd(0) < t)]\\\n",
    "               .dropna(subset=x_cols+[y_col])\n",
    "    df_test = df_tmp[(df_tmp['ANNDATS_q3']>df_tmp['YearMonth']) & (df_tmp['YearMonth'] == t)].dropna(subset=x_cols+[y_col])\n",
    "\n",
    "    mdl = LinearRegression()\n",
    "\n",
    "    mdl.fit(df_train[x_cols], df_train[y_col])\n",
    "    y_pred = mdl.predict(df_test[x_cols])\n",
    "\n",
    "    forecast_q3 = pd.DataFrame({'permno':df_test['permno'],'YearMonth':df_test['YearMonth'],\n",
    "                                'So_q3':y_pred, 'AF_q3':df_test['EPS_ana_q3'], 'AE_q3':df_test['EPS_true_q3']})\n",
    "\n",
    "    ### Y1 ###\n",
    "    x_cols = ['EPS_ana_y1','earnings_pos_l1_y1','nege_l1_y1','acc_n','acc_p','ag','dd','bm','prc','div']\n",
    "    y_col = 'EPS_true_y1'\n",
    "\n",
    "    df_train = df_tmp[(df_tmp['YearMonth'] < t) & (df_tmp['YearMonth'] >= t - MonthEnd(12)) & (df_tmp['ANNDATS_y1']  + MonthEnd(0) < t)]\\\n",
    "               .dropna(subset=x_cols+[y_col])\n",
    "    df_test = df_tmp[(df_tmp['ANNDATS_y1']>df_tmp['YearMonth']) & (df_tmp['YearMonth'] == t)].dropna(subset=x_cols+[y_col])\n",
    "\n",
    "    mdl = LinearRegression()\n",
    "\n",
    "    mdl.fit(df_train[x_cols], df_train[y_col])\n",
    "    y_pred = mdl.predict(df_test[x_cols])\n",
    "\n",
    "    mdl_sm = sm.OLS(df_train[y_col], sm.add_constant(df_train[x_cols])).fit()\n",
    "    params[t] = mdl_sm.params\n",
    "    t_values[t] = mdl_sm.tvalues\n",
    "\n",
    "    forecast_y1 = pd.DataFrame({'permno':df_test['permno'],'YearMonth':df_test['YearMonth'],\n",
    "                                'So_y1':y_pred, 'AF_y1':df_test['EPS_ana_y1'], 'AE_y1':df_test['EPS_true_y1']})\n",
    "\n",
    "    ### Y2 ###\n",
    "    x_cols = ['EPS_ana_y2','earnings_pos_l1_y1','nege_l1_y1','acc_n','acc_p','ag','dd','bm','prc','div']\n",
    "    y_col = 'EPS_true_y2'\n",
    "\n",
    "    df_train = df_tmp[(df_tmp['YearMonth'] < t) & (df_tmp['YearMonth'] >= t - MonthEnd(24)) & (df_tmp['ANNDATS_y2']  + MonthEnd(0) < t)]\\\n",
    "               .dropna(subset=x_cols+[y_col])\n",
    "    df_test = df_tmp[(df_tmp['ANNDATS_y2']>df_tmp['YearMonth']) & (df_tmp['YearMonth'] == t)].dropna(subset=x_cols+[y_col])\n",
    "\n",
    "    mdl = LinearRegression()\n",
    "\n",
    "    mdl.fit(df_train[x_cols], df_train[y_col])\n",
    "    y_pred = mdl.predict(df_test[x_cols])\n",
    "\n",
    "    forecast_y2 = pd.DataFrame({'permno':df_test['permno'],'YearMonth':df_test['YearMonth'],\n",
    "                                'So_y2':y_pred, 'AF_y2':df_test['EPS_ana_y2'], 'AE_y2':df_test['EPS_true_y2']})\n",
    "\n",
    "    forecast.append(reduce(lambda x,y: pd.merge(x,y,\n",
    "                                       on=['permno','YearMonth'],\n",
    "                                       how='outer'),\n",
    "                 [forecast_q1,forecast_q2,forecast_q3,\n",
    "                  forecast_y1,forecast_y2]))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "638mzFpZ6sDZ"
   },
   "outputs": [],
   "source": [
    "forecast_all = pd.concat(forecast,axis=0).reset_index(drop=True)\n",
    "forecast_all.to_parquet('../data/Results/So_eps_AF.parquet')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
